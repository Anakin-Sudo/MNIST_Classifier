{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-25T00:58:05.823371200Z",
     "start_time": "2025-09-25T00:58:05.785532200Z"
    }
   },
   "id": "165909c6be714088"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training samples: 49800, Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 1. Choose device: GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 2. Define transforms: convert to tensor + normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # convert image to tensor\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # mean and std for MNIST\n",
    "])\n",
    "\n",
    "# 3. Download datasets\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "train_size = int(0.83 * len(train_dataset))\n",
    "val_size= len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# 4. DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1000, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-24T23:33:51.872399400Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Fully connected (linear) layers\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # after pooling twice\n",
    "        self.fc2 = nn.Linear(128, 10)          # 10 digits\n",
    "\n",
    "        # Pooling layer (reduce size by factor of 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First conv block: conv -> BN -> ReLU -> pool\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        \n",
    "        # Second conv block\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # logits, no softmax (CrossEntropyLoss expects raw logits)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate model and send to GPU if available\n",
    "model = CNN().to(device)\n",
    "print(model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-24T23:33:52.249928100Z",
     "start_time": "2025-09-24T23:33:52.081455400Z"
    }
   },
   "id": "f25e64d0953a3a35"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Alternative definition style using nn.Sequential\n",
    "\n",
    "class CNN_Sequential(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Sequential, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 28 -> 14\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)   # 14 -> 7\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)  # raw logits\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)  # flatten\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-24T23:33:52.249928100Z",
     "start_time": "2025-09-24T23:33:52.179012500Z"
    }
   },
   "id": "4506f48c1f78d098"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Loss function: CrossEntropy for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: Adam with learning rate 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-24T23:33:52.249928100Z",
     "start_time": "2025-09-24T23:33:52.187040300Z"
    }
   },
   "id": "523b9101d365bf4a"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on this batch: 2.3244516849517822\n"
     ]
    }
   ],
   "source": [
    "# Get one batch of data from the train_loader\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Move them to GPU if available\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# 1. Zero gradients (clear previous updates)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 2. Forward pass: get raw logits\n",
    "outputs = model(images)  # shape [64, 10]\n",
    "\n",
    "# 3. Compute loss\n",
    "loss = criterion(outputs, labels)\n",
    "\n",
    "print(\"Loss on this batch:\", loss.item())\n",
    "\n",
    "# 4. Backward pass: compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# 5. Update weights\n",
    "optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-24T23:33:52.469957800Z",
     "start_time": "2025-09-24T23:33:52.218634500Z"
    }
   },
   "id": "2a94ab68bc52e5f0"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Train Loss: 0.1254, Train Acc: 96.13% | Val Loss: 0.0797, Val Acc: 97.48%\n",
      "Epoch [2/5] Train Loss: 0.0476, Train Acc: 98.54% | Val Loss: 0.0517, Val Acc: 98.52%\n",
      "Epoch [3/5] Train Loss: 0.0330, Train Acc: 98.92% | Val Loss: 0.0481, Val Acc: 98.60%\n",
      "Epoch [4/5] Train Loss: 0.0277, Train Acc: 99.11% | Val Loss: 0.0491, Val Acc: 98.34%\n",
      "Epoch [5/5] Train Loss: 0.0227, Train Acc: 99.29% | Val Loss: 0.0442, Val Acc: 98.79%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5  # you can increase later\n",
    "patience=3  # for early stopping\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- Training ----\n",
    "    model.train()  # set model to training mode (important for BatchNorm, Dropout)\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # 1. Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 2. Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 3. Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 4. Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Track accuracy on training batch\n",
    "        _, predicted = torch.max(outputs, 1)   # get class with highest logit\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # ---- validation loop ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%\")\n",
    "\n",
    "    # ---- Early stopping condition ----\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        # you might also save the model here\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-24T23:40:05.717048Z",
     "start_time": "2025-09-24T23:33:52.375700800Z"
    }
   },
   "id": "e6e56c554d555bc6"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 10]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "all_probs = []\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        batch_probs = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(batch_probs, 1)\n",
    "\n",
    "        # Append to lists\n",
    "        all_probs.append(batch_probs.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "        all_predictions.append(predicted.cpu())\n",
    "\n",
    "# Concatenate all batches into big tensors\n",
    "all_probs = torch.cat(all_probs, dim=0)     # shape [N, 10]\n",
    "all_labels = torch.cat(all_labels, dim=0)   # shape [N]\n",
    "all_predictions = torch.cat(all_predictions, dim=0)  # shape [N]\n",
    "\n",
    "print(all_probs.shape, all_labels.shape)  # should be [10000, 10], [10000]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-25T00:57:21.486793800Z",
     "start_time": "2025-09-25T00:57:11.310707400Z"
    }
   },
   "id": "a7d1540f79f640f4"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9881\n",
      "Precision: 0.9882578045645921\n",
      "Recall: 0.9879201065250388\n",
      "F1: 0.9880378500849171\n",
      "ROC AUC: 0.9999345951652415\n"
     ]
    }
   ],
   "source": [
    "y_true = all_labels.numpy()\n",
    "y_pred = all_predictions.numpy()\n",
    "y_prob = all_probs.numpy()\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"Precision:\", precision_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"Recall:\", recall_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"F1:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_true, y_prob, multi_class=\"ovr\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-25T00:59:57.482895600Z",
     "start_time": "2025-09-25T00:59:57.310536700Z"
    }
   },
   "id": "909cf671b70ef107"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mnist_cnn.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-25T13:39:02.459579900Z",
     "start_time": "2025-09-25T13:39:02.390695900Z"
    }
   },
   "id": "9fc9742f56eaf0e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a0de8d8ee5d588ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
